"""
Script para analizar el PDF antes de la ingesta
Ejecutar en Google Colab o localmente con: python scripts/analizar_pdf.py
"""

import sys
import os
from pathlib import Path

# Agregar el directorio ra√≠z al path
sys.path.insert(0, str(Path(__file__).parent.parent))

def analyze_pdf(file_path: str):
    """
    Analizar PDF y mostrar estad√≠sticas importantes
    """
    try:
        import PyPDF2
        import re
        
        print("=" * 60)
        print("üìÑ AN√ÅLISIS DEL PDF")
        print("=" * 60)
        
        # Abrir PDF
        with open(file_path, 'rb') as f:
            pdf_reader = PyPDF2.PdfReader(f)
            total_pages = len(pdf_reader.pages)
            
            print(f"\nüìä INFORMACI√ìN B√ÅSICA:")
            print(f"   Archivo: {os.path.basename(file_path)}")
            print(f"   Total de p√°ginas: {total_pages}")
            
            # Extraer texto de todas las p√°ginas
            print(f"\nüìñ Extrayendo texto de todas las p√°ginas...")
            all_text = ""
            pages_with_text = 0
            pages_text_length = []
            
            for page_num, page in enumerate(pdf_reader.pages, 1):
                try:
                    page_text = page.extract_text()
                    if page_text.strip():
                        pages_with_text += 1
                        text_len = len(page_text)
                        pages_text_length.append(text_len)
                        all_text += f"\n--- P√°gina {page_num} ---\n{page_text}"
                        
                        if page_num % 20 == 0:
                            print(f"   Procesadas {page_num}/{total_pages} p√°ginas...")
                except Exception as e:
                    print(f"   ‚ö†Ô∏è Error en p√°gina {page_num}: {e}")
                    continue
            
            print(f"\n‚úÖ Extracci√≥n completada:")
            print(f"   P√°ginas con texto: {pages_with_text}/{total_pages}")
            print(f"   Total de caracteres: {len(all_text):,}")
            print(f"   Promedio por p√°gina: {len(all_text) // pages_with_text if pages_with_text > 0 else 0:,} caracteres")
            
            # Analizar estructura
            print(f"\nüîç AN√ÅLISIS DE ESTRUCTURA:")
            
            # Buscar art√≠culos
            article_pattern = re.compile(r'(?i)(?:art[i√≠]culo|art\.?)\s+(\d+)', re.IGNORECASE)
            articles_found = article_pattern.findall(all_text)
            unique_articles = sorted(set(articles_found), key=lambda x: int(x) if x.isdigit() else 9999)
            
            print(f"   Art√≠culos detectados: {len(unique_articles)}")
            if unique_articles:
                print(f"   Rango: Art√≠culo {unique_articles[0]} - Art√≠culo {unique_articles[-1]}")
                print(f"   Primeros 10: {', '.join(unique_articles[:10])}")
                if len(unique_articles) > 10:
                    print(f"   √öltimos 10: {', '.join(unique_articles[-10:])}")
            
            # Buscar secciones
            section_pattern = re.compile(r'(?i)(?:secci[o√≥]n|cap[i√≠]tulo|t[i√≠]tulo)\s+([IVX\d]+)', re.IGNORECASE)
            sections_found = section_pattern.findall(all_text)
            print(f"   Secciones/Cap√≠tulos detectados: {len(set(sections_found))}")
            
            # Analizar distribuci√≥n de texto
            print(f"\nüìä DISTRIBUCI√ìN DE CONTENIDO:")
            if pages_text_length:
                avg_page_len = sum(pages_text_length) / len(pages_text_length)
                min_page_len = min(pages_text_length)
                max_page_len = max(pages_text_length)
                print(f"   P√°ginas con texto: {pages_with_text}")
                print(f"   Promedio por p√°gina: {avg_page_len:.0f} caracteres")
                print(f"   M√≠nimo: {min_page_len} caracteres")
                print(f"   M√°ximo: {max_page_len} caracteres")
            
            # Estimar chunks esperados
            print(f"\nüéØ ESTIMACI√ìN DE CHUNKS:")
            
            # Asumiendo chunk_size de ~1000 tokens (~800 caracteres en espa√±ol)
            chunk_size_chars = 800
            estimated_chunks_simple = len(all_text) // chunk_size_chars
            estimated_chunks_with_overlap = int(len(all_text) / (chunk_size_chars * 0.8))  # Con overlap
            
            print(f"   Con chunking simple (800 chars): ~{estimated_chunks_simple} chunks")
            print(f"   Con chunking + overlap (20%): ~{estimated_chunks_with_overlap} chunks")
            print(f"   Con chunking sem√°ntico: ~{estimated_chunks_with_overlap * 0.7:.0f}-{estimated_chunks_with_overlap * 1.3:.0f} chunks")
            print(f"   (El sem√°ntico puede agrupar o dividir m√°s)")
            
            # Verificar si hay mucho texto en metadata vs contenido
            print(f"\n‚ö†Ô∏è VERIFICACIONES:")
            
            # Buscar patrones que puedan indicar problemas
            empty_pages = total_pages - pages_with_text
            if empty_pages > 0:
                print(f"   ‚ö†Ô∏è {empty_pages} p√°ginas sin texto (pueden ser im√°genes o portadas)")
            
            # Verificar si hay mucho texto repetitivo
            words = all_text.split()
            unique_words = set(words)
            if len(words) > 0:
                uniqueness_ratio = len(unique_words) / len(words)
                if uniqueness_ratio < 0.3:
                    print(f"   ‚ö†Ô∏è Mucho texto repetitivo (ratio: {uniqueness_ratio:.2f})")
            
            # Buscar el art√≠culo 52 espec√≠ficamente
            print(f"\nüîç B√öSQUEDA ESPEC√çFICA:")
            if '52' in unique_articles:
                article_52_pattern = re.compile(r'(?i)(?:art[i√≠]culo|art\.?)\s+52[^\d]', re.IGNORECASE)
                matches = article_52_pattern.findall(all_text)
                print(f"   ‚úÖ Art√≠culo 52 encontrado en el texto")
                print(f"   Menciones: {len(matches)}")
                
                # Buscar contexto del art√≠culo 52
                article_52_context = re.search(
                    r'(?i)(?:art[i√≠]culo|art\.?)\s+52[^\d].{0,500}',
                    all_text,
                    re.DOTALL
                )
                if article_52_context:
                    preview = article_52_context.group(0)[:200].replace('\n', ' ')
                    print(f"   Preview: {preview}...")
            else:
                print(f"   ‚ùå Art√≠culo 52 NO encontrado en el texto")
                print(f"   (Puede estar escrito de forma diferente)")
            
            # Recomendaciones
            print(f"\nüí° RECOMENDACIONES:")
            
            if estimated_chunks_simple < 100 and total_pages > 50:
                print(f"   ‚ö†Ô∏è ADVERTENCIA: Se esperar√≠an m√°s chunks para un PDF de {total_pages} p√°ginas")
                print(f"   Posibles causas:")
                print(f"   - El PDF tiene muchas im√°genes y poco texto")
                print(f"   - El texto est√° en formato de imagen (OCR necesario)")
                print(f"   - El chunking sem√°ntico est√° agrupando demasiado")
            
            if len(unique_articles) < 10 and total_pages > 50:
                print(f"   ‚ö†Ô∏è Pocos art√≠culos detectados para un documento grande")
                print(f"   Puede que el formato de art√≠culos sea diferente")
            
            print(f"\n‚úÖ An√°lisis completado")
            print("=" * 60)
            
            return {
                'total_pages': total_pages,
                'pages_with_text': pages_with_text,
                'total_characters': len(all_text),
                'articles_found': len(unique_articles),
                'estimated_chunks': estimated_chunks_with_overlap
            }
            
    except ImportError:
        print("‚ùå Error: PyPDF2 no est√° instalado")
        print("   Instala con: pip install PyPDF2")
        return None
    except Exception as e:
        print(f"‚ùå Error analizando PDF: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    # Buscar el PDF en la carpeta
    pdf_folder = Path(__file__).parent.parent / "documento que usar√© en la ingesta"
    pdf_file = pdf_folder / "MANUAL DE CONVIVENCIA ESCOLAR ROLDANISTA 2023.pdf"
    
    if pdf_file.exists():
        print(f"üìÅ Encontrado: {pdf_file}")
        analyze_pdf(str(pdf_file))
    else:
        print(f"‚ùå No se encontr√≥ el PDF en: {pdf_folder}")
        print(f"\nüí° Para usar este script:")
        print(f"   1. Ejecuta: python scripts/analizar_pdf.py")
        print(f"   2. O copia el c√≥digo a Google Colab")
        print(f"   3. O proporciona la ruta del PDF como argumento")

