{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ ARB-BOT ‚Äî INGESTA MEJORADA V2.0\n",
        "\n",
        "**Cambios principales:**\n",
        "- ‚úÖ Usa `pdfplumber` (mejor que PyPDF2)\n",
        "- ‚úÖ Funci√≥n `fix_broken_words()` que repara palabras cortadas\n",
        "- ‚úÖ Limpieza autom√°tica en todo el proceso\n",
        "\n",
        "**Antes de ejecutar:**\n",
        "1. Sube el PDF: `MANUAL DE CONVIVENCIA ESCOLAR ROLDANISTA 2023.pdf`\n",
        "2. Ejecuta celda por celda\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) Instalar dependencias\n",
        "!pip install -q supabase sentence-transformers psycopg2-binary python-docx\n",
        "print('‚úÖ Dependencias instaladas')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2) Imports\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import psycopg2\n",
        "from psycopg2.extras import execute_values\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from docx import Document  # Para leer archivos Word\n",
        "from datetime import datetime, timezone\n",
        "from pathlib import Path\n",
        "\n",
        "print('‚úÖ Imports listos')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3) Configuraci√≥n - VERIFICA TUS CREDENCIALES DE SUPABASE\n",
        "# ============================================\n",
        "# Estas son las credenciales de conexi√≥n DIRECTA a PostgreSQL de Supabase\n",
        "# (No usamos el cliente de Supabase, sino conexi√≥n directa a la BD)\n",
        "# ============================================\n",
        "\n",
        "DB_HOST = \"aws-1-us-east-1.pooler.supabase.com\"\n",
        "DB_NAME = \"postgres\"\n",
        "DB_USER = \"postgres.ympekltzqzlsbdgbzbpz\"\n",
        "DB_PASS = \"Z32pp23z$$1124$$\"\n",
        "DB_PORT = \"6543\"\n",
        "\n",
        "SCHEMA = \"vecs\"\n",
        "TABLE = \"arbot_documents\"\n",
        "\n",
        "# ‚ö†Ô∏è CAMBIA EL NOMBRE DEL ARCHIVO SI ES DIFERENTE\n",
        "WORD_FILE = \"MANUAL DE CONVIVENCIA ESCOLAR ROLDANISTA 2023.docx\"\n",
        "\n",
        "def get_connection():\n",
        "    \"\"\"Conecta directamente a PostgreSQL de Supabase usando psycopg2.\"\"\"\n",
        "    return psycopg2.connect(\n",
        "        host=DB_HOST, \n",
        "        dbname=DB_NAME, \n",
        "        user=DB_USER, \n",
        "        password=DB_PASS, \n",
        "        port=DB_PORT\n",
        "    )\n",
        "\n",
        "# Test conexi√≥n\n",
        "print('üîó Probando conexi√≥n a Supabase PostgreSQL...')\n",
        "try:\n",
        "    conn_test = get_connection()\n",
        "    conn_test.close()\n",
        "    print('‚úÖ Conexi√≥n a Supabase OK')\n",
        "except Exception as e:\n",
        "    print(f'‚ùå Error de conexi√≥n: {e}')\n",
        "    print('   Verifica que las credenciales sean correctas.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4) üîß FUNCI√ìN MEJORADA: Limpiar palabras cortadas (sin ser agresiva)\n",
        "def fix_broken_words(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Repara palabras cortadas por mal OCR/extracci√≥n.\n",
        "    SOLO junta cuando hay UNA sola letra seguida de espacio y otra letra.\n",
        "    Ejemplos: 'institu ci√≥n' ‚Üí 'instituci√≥n'\n",
        "    NO debe juntar palabras completas.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return text\n",
        "    \n",
        "    # Patr√≥n CONSERVADOR: solo junta cuando hay 1-2 letras sueltas\n",
        "    # Ejemplo: \"a ci√≥n\" ‚Üí \"aci√≥n\", \"Rol d√°n\" ‚Üí \"Rold√°n\"\n",
        "    \n",
        "    # Caso 1: Una letra min√∫scula suelta + espacio + palabra\n",
        "    # \"a ci√≥n\" ‚Üí \"aci√≥n\"\n",
        "    text = re.sub(r'\\b([a-z√°√©√≠√≥√∫√±√º]{1,2}) ([a-z√°√©√≠√≥√∫√±√º])', r'\\1\\2', text)\n",
        "    \n",
        "    # Caso 2: Palabra + espacio + una letra min√∫scula suelta\n",
        "    # \"instituc i√≥n\" ‚Üí \"instituci√≥n\"  \n",
        "    text = re.sub(r'([a-z√°√©√≠√≥√∫√±√º]) ([a-z√°√©√≠√≥√∫√±√º]{1,3})\\b', r'\\1\\2', text)\n",
        "    \n",
        "    # Caso 3: May√∫scula + espacio + min√∫scula (inicio de palabra cortada)\n",
        "    # \"Rol d√°n\" ‚Üí \"Rold√°n\"\n",
        "    text = re.sub(r'([A-Z√Å√â√ç√ì√ö√ë√ú][a-z√°√©√≠√≥√∫√±√º]*) ([a-z√°√©√≠√≥√∫√±√º]{1,3})\\b', r'\\1\\2', text)\n",
        "    \n",
        "    # NO tocar may√∫sculas completas para no pegar t√≠tulos\n",
        "    \n",
        "    # Limpiar espacios m√∫ltiples\n",
        "    text = re.sub(r'  +', ' ', text)\n",
        "    \n",
        "    return text.strip()\n",
        "\n",
        "# Test de la funci√≥n\n",
        "print(\"üß™ Test de limpieza (versi√≥n conservadora):\")\n",
        "tests = [\n",
        "    (\"institu ci√≥n\", \"instituci√≥n\"),\n",
        "    (\"ESTUDIANTE S\", \"ESTUDIANTE S\"),  # NO debe cambiar may√∫sculas\n",
        "    (\"Rol d√°n\", \"Rold√°n\"),\n",
        "    (\"derech o al debido proces o\", \"derecho al debido proceso\"),\n",
        "    (\"MANUAL DE CONVIVENCIA\", \"MANUAL DE CONVIVENCIA\"),  # NO debe cambiar\n",
        "]\n",
        "for original, esperado in tests:\n",
        "    resultado = fix_broken_words(original)\n",
        "    status = \"‚úÖ\" if resultado == esperado else \"‚ùå\"\n",
        "    print(f\"  {status} '{original}' ‚Üí '{resultado}' (esperado: '{esperado}')\")\n",
        "\n",
        "print('\\n‚úÖ Funci√≥n de limpieza lista')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5) Funci√≥n para leer archivo Word (.docx)\n",
        "# ¬°Mucho m√°s limpio que PDF!\n",
        "\n",
        "def read_word_extract_text(docx_path: str):\n",
        "    \"\"\"\n",
        "    Extrae texto de un archivo Word (.docx).\n",
        "    El texto viene LIMPIO, sin problemas de OCR ni palabras cortadas.\n",
        "    \"\"\"\n",
        "    doc = Document(docx_path)\n",
        "    paragraphs = []\n",
        "    full_text = \"\"\n",
        "    \n",
        "    print(f\"üìÑ Leyendo documento Word: {docx_path}\")\n",
        "    \n",
        "    for i, para in enumerate(doc.paragraphs):\n",
        "        text = para.text.strip()\n",
        "        if text:  # Solo p√°rrafos con contenido\n",
        "            paragraphs.append({\"index\": i, \"text\": text})\n",
        "            full_text += text + \"\\n\\n\"\n",
        "    \n",
        "    print(f'‚úÖ Documento le√≠do: {len(paragraphs)} p√°rrafos con texto')\n",
        "    \n",
        "    # Mostrar preview\n",
        "    print(\"\\nüìã Preview (primeros 500 caracteres):\")\n",
        "    print(\"-\"*40)\n",
        "    print(full_text[:500])\n",
        "    print(\"-\"*40)\n",
        "    \n",
        "    return {\"text\": full_text, \"paragraphs\": paragraphs}\n",
        "\n",
        "print('‚úÖ Funci√≥n de extracci√≥n de Word lista')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6) Utilidades\n",
        "def clean_text(s):\n",
        "    \"\"\"Limpia espacios extra.\"\"\"\n",
        "    if not s: return s\n",
        "    return re.sub(r'\\s+', ' ', s).strip()\n",
        "\n",
        "print('‚úÖ Utilidades listas')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7) Utilidades y Chunking jer√°rquico\n",
        "def clean_text(s):\n",
        "    if not s: return s\n",
        "    s = fix_broken_words(s)\n",
        "    return re.sub(r'\\s+', ' ', s).strip()\n",
        "\n",
        "def chunk_hierarchical_legal(full_text, max_tokens=1600):\n",
        "    lines = full_text.split('\\n')\n",
        "    chunks = []\n",
        "    current = {'title': None, 'chapter': None, 'article': None, 'text_lines': []}\n",
        "\n",
        "    title_pat = re.compile(r'(?i)^(T√çTULO|TITULO)\\b')\n",
        "    chapter_pat = re.compile(r'(?i)^(CAP√çTULO|CAPITULO)\\b')\n",
        "    article_pat = re.compile(r'(?i)^(ART√çCULO|ARTICULO|Art√≠culo|Articulo)\\s*\\d+')\n",
        "\n",
        "    def save():\n",
        "        txt = '\\n'.join(current['text_lines']).strip()\n",
        "        if txt:\n",
        "            cleaned = clean_text(txt)\n",
        "            chunks.append({'text': cleaned, 'meta': {k: current.get(k) for k in ('title','chapter','article')}})\n",
        "\n",
        "    for line in lines:\n",
        "        s = line.strip()\n",
        "        if not s:\n",
        "            current['text_lines'].append('')\n",
        "            continue\n",
        "        if title_pat.match(s):\n",
        "            save()\n",
        "            current = {'title': s, 'chapter': None, 'article': None, 'text_lines': [s]}\n",
        "            continue\n",
        "        if chapter_pat.match(s):\n",
        "            save()\n",
        "            current['chapter'] = s\n",
        "            current['text_lines'] = [s]\n",
        "            continue\n",
        "        if article_pat.match(s):\n",
        "            save()\n",
        "            current['article'] = s\n",
        "            current['text_lines'] = [s]\n",
        "            continue\n",
        "        current['text_lines'].append(s)\n",
        "\n",
        "    save()\n",
        "    \n",
        "    for i, c in enumerate(chunks):\n",
        "        c['meta']['chunk_index'] = i\n",
        "        c['meta']['ingestion_date'] = datetime.now(timezone.utc).isoformat()\n",
        "    \n",
        "    print(f'‚úÖ Chunking: {len(chunks)} chunks')\n",
        "    return chunks\n",
        "\n",
        "print('‚úÖ Chunking listo')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8) Modelo de embeddings\n",
        "print('üß† Cargando modelo de embeddings...')\n",
        "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
        "print(f'‚úÖ Modelo cargado ‚Äî dimensi√≥n: {model.get_sentence_embedding_dimension()}')\n",
        "\n",
        "def make_embeddings(texts, batch_size=32):\n",
        "    embs = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        arr = model.encode(batch, show_progress_bar=True, convert_to_numpy=True)\n",
        "        embs.extend([v.tolist() for v in arr])\n",
        "    return embs\n",
        "\n",
        "print('‚úÖ Embeddings listos')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 9) Subir a Supabase\n",
        "def upload_chunks(chunks, conn):\n",
        "    cur = conn.cursor()\n",
        "    \n",
        "    records = []\n",
        "    for i, c in enumerate(chunks):\n",
        "        text = clean_text(c['text'])\n",
        "        if not text: continue\n",
        "        meta = c.get('meta', {})\n",
        "        chunk_id = f\"{meta.get('file', 'doc')}_{meta.get('chunk_index', i)}\"\n",
        "        records.append((chunk_id, text, json.dumps(meta)))\n",
        "    \n",
        "    print(f'üìä Generando embeddings para {len(records)} chunks...')\n",
        "    embeddings = make_embeddings([r[1] for r in records])\n",
        "    \n",
        "    print(f'üì§ Subiendo a Supabase...')\n",
        "    insert_sql = f\"\"\"\n",
        "        INSERT INTO {SCHEMA}.{TABLE} (id, vec, text, metadata) \n",
        "        VALUES %s \n",
        "        ON CONFLICT (id) DO UPDATE SET vec=EXCLUDED.vec, text=EXCLUDED.text, metadata=EXCLUDED.metadata\n",
        "    \"\"\"\n",
        "    \n",
        "    to_insert = [(rid, '['+','.join(map(str,emb))+']', txt, meta) \n",
        "                 for (rid, txt, meta), emb in zip(records, embeddings)]\n",
        "    \n",
        "    execute_values(cur, insert_sql, to_insert, template=\"(%s, %s::vector, %s, %s::jsonb)\")\n",
        "    conn.commit()\n",
        "    print(f'‚úÖ Subidos {len(to_insert)} chunks')\n",
        "    cur.close()\n",
        "\n",
        "print('‚úÖ Funci√≥n de subida lista')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 10) üöÄ EJECUTAR INGESTA DESDE WORD\n",
        "# ‚ö†Ô∏è IMPORTANTE: Este script BORRA todos los datos anteriores de Supabase\n",
        "\n",
        "if not os.path.exists(WORD_FILE):\n",
        "    print(f\"‚ùó No se encuentra: {WORD_FILE}\")\n",
        "    print(\"   Sube el archivo Word (.docx) antes de ejecutar esta celda.\")\n",
        "    print(\"\\n   üìÅ Clic en el √≠cono de carpeta (izquierda) ‚Üí Subir archivo\")\n",
        "else:\n",
        "    print('='*50)\n",
        "    print('üöÄ INGESTA DESDE WORD (.docx)')\n",
        "    print('='*50)\n",
        "    \n",
        "    # 1) Extraer texto del Word (¬°viene limpio!)\n",
        "    print('\\nüìÑ Paso 1: Leyendo archivo Word...')\n",
        "    data = read_word_extract_text(WORD_FILE)\n",
        "    \n",
        "    # 2) Generar chunks\n",
        "    print('\\n‚úÇÔ∏è Paso 2: Generando chunks por art√≠culos/cap√≠tulos...')\n",
        "    chunks = chunk_hierarchical_legal(data['text'])\n",
        "    for i, c in enumerate(chunks):\n",
        "        c['meta']['file'] = WORD_FILE\n",
        "    \n",
        "    # 3) Conectar a Supabase\n",
        "    print('\\nüîó Paso 3: Conectando a Supabase...')\n",
        "    conn = get_connection()\n",
        "    \n",
        "    # 4) BORRAR DATOS ANTERIORES\n",
        "    print('\\nüóëÔ∏è Paso 4: BORRANDO datos anteriores de Supabase...')\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(f\"SELECT count(*) FROM {SCHEMA}.{TABLE}\")\n",
        "    datos_anteriores = cur.fetchone()[0]\n",
        "    print(f'   Datos anteriores: {datos_anteriores} chunks')\n",
        "    cur.execute(f\"DELETE FROM {SCHEMA}.{TABLE};\")\n",
        "    conn.commit()\n",
        "    cur.close()\n",
        "    print('   ‚úÖ Tabla limpiada completamente')\n",
        "    \n",
        "    # 5) Subir chunks con embeddings\n",
        "    print('\\nüì§ Paso 5: Subiendo chunks con embeddings...')\n",
        "    upload_chunks(chunks, conn)\n",
        "    \n",
        "    # 6) Verificar resultado\n",
        "    print('\\nüîç Paso 6: Verificando...')\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(f\"SELECT count(*) FROM {SCHEMA}.{TABLE}\")\n",
        "    total = cur.fetchone()[0]\n",
        "    cur.close()\n",
        "    conn.close()\n",
        "    \n",
        "    print('\\n' + '='*50)\n",
        "    print(f'‚úÖ INGESTA COMPLETADA EXITOSAMENTE')\n",
        "    print(f'   üìä Chunks subidos: {total}')\n",
        "    print(f'   üìÑ Fuente: Word (.docx) - texto limpio')\n",
        "    print('='*50)\n",
        "    print('\\n‚ö†Ô∏è Ahora ve a Railway y haz REDEPLOY de tu bot.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 11) üß™ TEST: Verificar el art√≠culo 52\n",
        "conn = get_connection()\n",
        "cur = conn.cursor()\n",
        "cur.execute(f\"\"\"\n",
        "    SELECT LEFT(text, 600) \n",
        "    FROM {SCHEMA}.{TABLE} \n",
        "    WHERE text ILIKE '%art√≠culo 52%' \n",
        "    LIMIT 1\n",
        "\"\"\")\n",
        "result = cur.fetchone()\n",
        "cur.close()\n",
        "conn.close()\n",
        "\n",
        "if result:\n",
        "    print('‚úÖ Art√≠culo 52 encontrado:')\n",
        "    print('-'*50)\n",
        "    print(result[0])\n",
        "    print('-'*50)\n",
        "    print('\\nüëÜ Verifica que el texto est√© LIMPIO (sin palabras cortadas)')\n",
        "else:\n",
        "    print('‚ö†Ô∏è No se encontr√≥ el art√≠culo 52')\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
