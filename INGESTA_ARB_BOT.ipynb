{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARB-BOT - INGESTA V2.0 (Word)\n",
    "\n",
    "**Antes de ejecutar:**\n",
    "1. Sube: `MANUAL DE CONVIVENCIA ESCOLAR ROLDANISTA 2023.docx`\n",
    "2. Ejecuta celda por celda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Instalar dependencias\n",
    "!pip install -q supabase sentence-transformers psycopg2-binary python-docx\n",
    "print('Dependencias instaladas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Imports\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from docx import Document\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "print('Imports listos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Configuracion\n",
    "\n",
    "DB_HOST = \"aws-1-us-east-1.pooler.supabase.com\"\n",
    "DB_NAME = \"postgres\"\n",
    "DB_USER = \"postgres.ympekltzqzlsbdgbzbpz\"\n",
    "DB_PASS = \"Z32pp23z$$1124$$\"\n",
    "DB_PORT = \"6543\"\n",
    "\n",
    "SCHEMA = \"vecs\"\n",
    "TABLE = \"arbot_documents\"\n",
    "\n",
    "WORD_FILE = \"MANUAL DE CONVIVENCIA ESCOLAR ROLDANISTA 2023.docx\"\n",
    "\n",
    "def get_connection():\n",
    "    return psycopg2.connect(\n",
    "        host=DB_HOST, \n",
    "        dbname=DB_NAME, \n",
    "        user=DB_USER, \n",
    "        password=DB_PASS, \n",
    "        port=DB_PORT\n",
    "    )\n",
    "\n",
    "print('Probando conexion...')\n",
    "try:\n",
    "    conn_test = get_connection()\n",
    "    conn_test.close()\n",
    "    print('Conexion OK')\n",
    "except Exception as e:\n",
    "    print(f'Error: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Funcion para leer Word (con paginas estimadas)\n",
    "\n",
    "def read_word_extract_text(docx_path):\n",
    "    doc = Document(docx_path)\n",
    "    paragraphs = []\n",
    "    full_text = \"\"\n",
    "    \n",
    "    print(f\"Leyendo: {docx_path}\")\n",
    "    \n",
    "    # Contar secciones (= paginas aproximadas)\n",
    "    total_sections = len(doc.sections)\n",
    "    total_paras = len(doc.paragraphs)\n",
    "    \n",
    "    for i, para in enumerate(doc.paragraphs):\n",
    "        text = para.text.strip()\n",
    "        if text:\n",
    "            # Estimar pagina basado en posicion\n",
    "            page_estimate = int((i / total_paras) * total_sections) + 1\n",
    "            paragraphs.append({\"index\": i, \"text\": text, \"page\": page_estimate})\n",
    "            full_text += text + \"\\n\\n\"\n",
    "    \n",
    "    print(f'Leidos: {len(paragraphs)} parrafos')\n",
    "    print(f'Secciones/Paginas: {total_sections}')\n",
    "    print(\"\\nPreview:\")\n",
    "    print(\"-\"*40)\n",
    "    print(full_text[:500])\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    return {\"text\": full_text, \"paragraphs\": paragraphs, \"total_pages\": total_sections}\n",
    "\n",
    "print('Funcion Word lista (con paginas)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Chunking jerarquico COMPLETO (con paragrafos y keywords)\n",
    "\n",
    "def extract_keywords(text, max_kw=5):\n",
    "    \"\"\"Extrae palabras clave del texto.\"\"\"\n",
    "    stopwords = {'el','la','los','las','de','del','en','con','por','para','que','se','su','sus','un','una','al','es','son','como','este','esta','estos','estas','lo','le','les','ser','hacer','puede','debe','cada','todo','toda','todos','todas','sin','sobre','entre','desde','hasta','cuando','donde','porque','esto','eso','asi','mas','menos','muy','bien','mal','solo','mismo','misma','otros','otras','otro','otra','hay','han','sido','esta','estan','tiene','tienen','cual','cuales','segun','mediante','dentro','fuera','antes','despues','durante','siempre','nunca','tambien','pero','sino','aunque','mientras','siendo','sera','seran','fueron','fue'}\n",
    "    words = re.findall(r'\\b[a-záéíóúñ]{4,}\\b', text.lower())\n",
    "    freq = {}\n",
    "    for w in words:\n",
    "        if w not in stopwords:\n",
    "            freq[w] = freq.get(w, 0) + 1\n",
    "    return [w for w, _ in sorted(freq.items(), key=lambda x: x[1], reverse=True)[:max_kw]]\n",
    "\n",
    "def chunk_hierarchical_legal(full_text):\n",
    "    lines = full_text.split('\\n')\n",
    "    chunks = []\n",
    "    current = {'title': None, 'chapter': None, 'article': None, 'paragraph': None, 'text_lines': []}\n",
    "\n",
    "    title_pat = re.compile(r'(?i)^(TITULO|TÍTULO)\\s*[IVXLCDM\\d]+')\n",
    "    chapter_pat = re.compile(r'(?i)^(CAPITULO|CAPÍTULO)\\s*[IVXLCDM\\d]+')\n",
    "    article_pat = re.compile(r'(?i)^(ARTICULO|ARTÍCULO|Articulo|Artículo)\\s*\\d+')\n",
    "    paragraph_pat = re.compile(r'(?i)^(PARAGRAFO|PARÁGRAFO|Paragrafo|Parágrafo)\\s*\\d*')\n",
    "\n",
    "    def save():\n",
    "        txt = '\\n'.join(current['text_lines']).strip()\n",
    "        if txt:\n",
    "            meta = {k: current.get(k) for k in ('title','chapter','article','paragraph')}\n",
    "            meta['keywords'] = extract_keywords(txt)\n",
    "            meta['chunk_tokens'] = len(txt.split())\n",
    "            chunks.append({'text': txt, 'meta': meta})\n",
    "\n",
    "    for line in lines:\n",
    "        s = line.strip()\n",
    "        if not s:\n",
    "            current['text_lines'].append('')\n",
    "            continue\n",
    "        if title_pat.match(s):\n",
    "            save()\n",
    "            current = {'title': s, 'chapter': None, 'article': None, 'paragraph': None, 'text_lines': [s]}\n",
    "            continue\n",
    "        if chapter_pat.match(s):\n",
    "            save()\n",
    "            current['chapter'] = s\n",
    "            current['paragraph'] = None\n",
    "            current['text_lines'] = [s]\n",
    "            continue\n",
    "        if article_pat.match(s):\n",
    "            save()\n",
    "            current['article'] = s\n",
    "            current['paragraph'] = None\n",
    "            current['text_lines'] = [s]\n",
    "            continue\n",
    "        if paragraph_pat.match(s):\n",
    "            save()\n",
    "            current['paragraph'] = s\n",
    "            current['text_lines'] = [s]\n",
    "            continue\n",
    "        current['text_lines'].append(s)\n",
    "\n",
    "    save()\n",
    "    \n",
    "    for i, c in enumerate(chunks):\n",
    "        c['meta']['chunk_index'] = i\n",
    "        c['meta']['ingestion_date'] = datetime.now(timezone.utc).isoformat()\n",
    "    \n",
    "    print(f'Chunking: {len(chunks)} chunks')\n",
    "    print(f'  - Con paragrafos detectados')\n",
    "    print(f'  - Con keywords extraidas')\n",
    "    print(f'  - Con chunk_tokens calculados')\n",
    "    return chunks\n",
    "\n",
    "print('Funcion chunking COMPLETO lista')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Modelo de embeddings\n",
    "\n",
    "print('Cargando modelo...')\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "print(f'Modelo cargado - dimension: {model.get_sentence_embedding_dimension()}')\n",
    "\n",
    "def make_embeddings(texts, batch_size=32):\n",
    "    embs = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        arr = model.encode(batch, show_progress_bar=True, convert_to_numpy=True)\n",
    "        embs.extend([v.tolist() for v in arr])\n",
    "    return embs\n",
    "\n",
    "print('Embeddings listos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Subir a Supabase\n",
    "\n",
    "def upload_chunks(chunks, conn):\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    records = []\n",
    "    for i, c in enumerate(chunks):\n",
    "        text = c['text'].strip()\n",
    "        if not text: continue\n",
    "        meta = c.get('meta', {})\n",
    "        chunk_id = f\"{meta.get('file', 'doc')}_{meta.get('chunk_index', i)}\"\n",
    "        records.append((chunk_id, text, json.dumps(meta)))\n",
    "    \n",
    "    print(f'Generando embeddings para {len(records)} chunks...')\n",
    "    embeddings = make_embeddings([r[1] for r in records])\n",
    "    \n",
    "    print('Subiendo a Supabase...')\n",
    "    insert_sql = f\"\"\"\n",
    "        INSERT INTO {SCHEMA}.{TABLE} (id, vec, text, metadata) \n",
    "        VALUES %s \n",
    "        ON CONFLICT (id) DO UPDATE SET vec=EXCLUDED.vec, text=EXCLUDED.text, metadata=EXCLUDED.metadata\n",
    "    \"\"\"\n",
    "    \n",
    "    to_insert = [(rid, '['+','.join(map(str,emb))+']', txt, meta) \n",
    "                 for (rid, txt, meta), emb in zip(records, embeddings)]\n",
    "    \n",
    "    execute_values(cur, insert_sql, to_insert, template=\"(%s, %s::vector, %s, %s::jsonb)\")\n",
    "    conn.commit()\n",
    "    print(f'Subidos {len(to_insert)} chunks')\n",
    "    cur.close()\n",
    "\n",
    "print('Funcion subida lista')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) EJECUTAR INGESTA\n",
    "\n",
    "if not os.path.exists(WORD_FILE):\n",
    "    print(f\"No se encuentra: {WORD_FILE}\")\n",
    "    print(\"Sube el archivo Word antes de ejecutar.\")\n",
    "else:\n",
    "    print('='*50)\n",
    "    print('INGESTA DESDE WORD')\n",
    "    print('='*50)\n",
    "    \n",
    "    print('\\nPaso 1: Leyendo Word...')\n",
    "    data = read_word_extract_text(WORD_FILE)\n",
    "    total_pages = data.get('total_pages', 192)\n",
    "    \n",
    "    print('\\nPaso 2: Generando chunks...')\n",
    "    chunks = chunk_hierarchical_legal(data['text'])\n",
    "    \n",
    "    # Asignar pagina estimada a cada chunk\n",
    "    total_chunks = len(chunks)\n",
    "    for i, c in enumerate(chunks):\n",
    "        c['meta']['file'] = WORD_FILE\n",
    "        c['meta']['page'] = int((i / total_chunks) * total_pages) + 1\n",
    "    \n",
    "    print(f'   Paginas asignadas (1 a {total_pages})')\n",
    "    \n",
    "    print('\\nPaso 3: Conectando...')\n",
    "    conn = get_connection()\n",
    "    \n",
    "    print('\\nPaso 4: Borrando datos anteriores...')\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(f\"SELECT count(*) FROM {SCHEMA}.{TABLE}\")\n",
    "    datos_anteriores = cur.fetchone()[0]\n",
    "    print(f'   Anteriores: {datos_anteriores} chunks')\n",
    "    cur.execute(f\"DELETE FROM {SCHEMA}.{TABLE};\")\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    print('   Tabla limpiada')\n",
    "    \n",
    "    print('\\nPaso 5: Subiendo chunks...')\n",
    "    upload_chunks(chunks, conn)\n",
    "    \n",
    "    print('\\nPaso 6: Verificando...')\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(f\"SELECT count(*) FROM {SCHEMA}.{TABLE}\")\n",
    "    total = cur.fetchone()[0]\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    \n",
    "    print('\\n' + '='*50)\n",
    "    print(f'INGESTA COMPLETADA - {total} chunks')\n",
    "    print(f'Con metadatos: title, chapter, article, paragraph,')\n",
    "    print(f'               page, keywords, chunk_tokens')\n",
    "    print('='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) TEST: Verificar articulo 52\n",
    "\n",
    "conn = get_connection()\n",
    "cur = conn.cursor()\n",
    "cur.execute(f\"\"\"\n",
    "    SELECT LEFT(text, 600) \n",
    "    FROM {SCHEMA}.{TABLE} \n",
    "    WHERE text ILIKE '%articulo 52%' OR text ILIKE '%artículo 52%'\n",
    "    LIMIT 1\n",
    "\"\"\")\n",
    "result = cur.fetchone()\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "if result:\n",
    "    print('Articulo 52 encontrado:')\n",
    "    print('-'*50)\n",
    "    print(result[0])\n",
    "    print('-'*50)\n",
    "else:\n",
    "    print('No se encontro el articulo 52')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
